{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "inline_rc = dict(mpl.rcParams)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CARGAMOS LOS DATOS DE yelp_academic_dataset_review.json"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                review_id                 user_id             business_id  \\\n",
      "0  Q1sbwvVQXV2734tPgoKj4Q  hG7b0MtEbXx5QzbzE6C_VA  ujmEBvifdJM6h6RLv4wQIg   \n",
      "1  GJXCdrto3ASJOqKeVWPi6Q  yXQM5uF2jS6es16SJzNHfg  NZnhc2sEQy3RmzKTZnqtwQ   \n",
      "2  2TzJjDVDEuAW6MR5Vuc1ug  n6-Gk65cPZL6Uz8qRm3NYw  WTqjgwHlXbSFevF32_DJVw   \n",
      "3  yi0R0Ugj_xUx_Nek0-_Qig  dacAIZ6fTM6mqwW5uxkskg  ikCg8xy5JIg_NGPx-MSIDA   \n",
      "4  11a8sVPMUFtaC7_ABRkmtw  ssoyf2_x0EQMed6fgHeMyQ  b1b1eb3uo-w561D0ZfCEiQ   \n",
      "\n",
      "   stars  useful  funny  cool  \\\n",
      "0    1.0       6      1     0   \n",
      "1    5.0       0      0     0   \n",
      "2    5.0       3      0     0   \n",
      "3    5.0       0      0     0   \n",
      "4    1.0       7      0     0   \n",
      "\n",
      "                                                text                 date  \n",
      "0  Total bill for this horrible service? Over $8G...  2013-05-07 04:34:36  \n",
      "1  I *adore* Travis at the Hard Rock's new Kelly ...  2017-01-14 21:30:33  \n",
      "2  I have to say that this office really has it t...  2016-11-09 20:09:03  \n",
      "3  Went in for a lunch. Steak sandwich was delici...  2018-01-09 20:56:38  \n",
      "4  Today was my second out of three sessions I ha...  2018-01-30 23:07:38  \n"
     ]
    }
   ],
   "source": [
    "reviews = []\n",
    "with open('data/yelp_academic_dataset_review.json', encoding=\"ansi\") as fl:\n",
    "    i=0\n",
    "    for review in fl:\n",
    "        reviews.append(json.loads(review))\n",
    "        i+=1\n",
    "        if i + 1 > 200000:\n",
    "            break\n",
    "\n",
    "df_review = pd.DataFrame(reviews)\n",
    "print(df_review.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CARGAMOS LOS DATOS DE yelp_academic_dataset_business.json"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              business_id                             name  \\\n",
      "0  1SWheh84yJXfytovILXOAQ       Arizona Biltmore Golf Club   \n",
      "1  QXAEGFB4oINsVuTFxEYKFQ       Emerald Chinese Restaurant   \n",
      "2  gnKjwL_1w79qoiV3IC_xQQ      Musashi Japanese Restaurant   \n",
      "3  xvX2CttrVhyG2z1dFg_0xw  Farmers Insurance - Paul Lorenz   \n",
      "4  HhyxOkGAM07SRYtlQ4wMFQ              Queen City Plumbing   \n",
      "\n",
      "                          address         city state postal_code   latitude  \\\n",
      "0     2818 E Camino Acequia Drive      Phoenix    AZ       85016  33.522143   \n",
      "1            30 Eglinton Avenue W  Mississauga    ON     L5R 3E7  43.605499   \n",
      "2       10110 Johnston Rd, Ste 15    Charlotte    NC       28210  35.092564   \n",
      "3   15655 W Roosevelt St, Ste 237     Goodyear    AZ       85338  33.455613   \n",
      "4  4209 Stuart Andrew Blvd, Ste F    Charlotte    NC       28217  35.190012   \n",
      "\n",
      "    longitude  stars  review_count  is_open  \\\n",
      "0 -112.018481    3.0             5        0   \n",
      "1  -79.652289    2.5           128        1   \n",
      "2  -80.859132    4.0           170        1   \n",
      "3 -112.395596    5.0             3        1   \n",
      "4  -80.887223    4.0             4        1   \n",
      "\n",
      "                                          attributes  \\\n",
      "0                           {'GoodForKids': 'False'}   \n",
      "1  {'RestaurantsReservations': 'True', 'GoodForMe...   \n",
      "2  {'GoodForKids': 'True', 'NoiseLevel': 'u'avera...   \n",
      "3                                               None   \n",
      "4  {'BusinessAcceptsBitcoin': 'False', 'ByAppoint...   \n",
      "\n",
      "                                          categories  \\\n",
      "0                                  Golf, Active Life   \n",
      "1  Specialty Food, Restaurants, Dim Sum, Imported...   \n",
      "2                  Sushi Bars, Restaurants, Japanese   \n",
      "3                      Insurance, Financial Services   \n",
      "4  Plumbing, Shopping, Local Services, Home Servi...   \n",
      "\n",
      "                                               hours  \n",
      "0                                               None  \n",
      "1  {'Monday': '9:0-0:0', 'Tuesday': '9:0-0:0', 'W...  \n",
      "2  {'Monday': '17:30-21:30', 'Wednesday': '17:30-...  \n",
      "3  {'Monday': '8:0-17:0', 'Tuesday': '8:0-17:0', ...  \n",
      "4  {'Monday': '7:0-23:0', 'Tuesday': '7:0-23:0', ...  \n"
     ]
    }
   ],
   "source": [
    "business = []\n",
    "with open('data/yelp_academic_dataset_business.json', encoding=\"ansi\") as fl:\n",
    "    for i, line in enumerate(fl):\n",
    "        business.append(json.loads(line))\n",
    "\n",
    "df_business = pd.DataFrame(business)\n",
    "print(df_business.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ELIMINAMOS LAS COLUMNAS INNECESARIAS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "df_review = df_review.drop([\"review_id\", \"user_id\", \"useful\", \"funny\", \"cool\", \"date\"], axis=1)\n",
    "df_business = df_business.drop([\"name\", \"address\", \"city\", \"state\", \"postal_code\", \"latitude\", \"longitude\", \"stars\", \"review_count\", \"is_open\", \"attributes\", \"hours\"], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "COMPROBAMOS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              business_id  stars  \\\n",
      "0  ujmEBvifdJM6h6RLv4wQIg    1.0   \n",
      "1  NZnhc2sEQy3RmzKTZnqtwQ    5.0   \n",
      "2  WTqjgwHlXbSFevF32_DJVw    5.0   \n",
      "3  ikCg8xy5JIg_NGPx-MSIDA    5.0   \n",
      "4  b1b1eb3uo-w561D0ZfCEiQ    1.0   \n",
      "\n",
      "                                                text  \n",
      "0  Total bill for this horrible service? Over $8G...  \n",
      "1  I *adore* Travis at the Hard Rock's new Kelly ...  \n",
      "2  I have to say that this office really has it t...  \n",
      "3  Went in for a lunch. Steak sandwich was delici...  \n",
      "4  Today was my second out of three sessions I ha...  \n",
      "              business_id                                         categories\n",
      "0  1SWheh84yJXfytovILXOAQ                                  Golf, Active Life\n",
      "1  QXAEGFB4oINsVuTFxEYKFQ  Specialty Food, Restaurants, Dim Sum, Imported...\n",
      "2  gnKjwL_1w79qoiV3IC_xQQ                  Sushi Bars, Restaurants, Japanese\n",
      "3  xvX2CttrVhyG2z1dFg_0xw                      Insurance, Financial Services\n",
      "4  HhyxOkGAM07SRYtlQ4wMFQ  Plumbing, Shopping, Local Services, Home Servi...\n"
     ]
    }
   ],
   "source": [
    "print(df_review.head())\n",
    "print(df_business.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preprocesado a lo fácil para poder trabajar con ello"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "X = []\n",
    "for r in df_review[\"text\"]:\n",
    "    txt = word_tokenize(r.lower())\n",
    "    newtxt = \"\"\n",
    "    for w in txt:\n",
    "        l = lemma.lemmatize(w)\n",
    "        newtxt += l+\" \"\n",
    "    X.append(newtxt)\n",
    "\n",
    "X = vectorizer.fit_transform(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lo que estoy haciendo realmente, hay que terminarlo pero con la opción de encima va bien"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "'from nltk.tokenize import word_tokenize\\nfrom nltk.stem import WordNetLemmatizer\\nbag = {}  #Menciones totales de cada palabra\\nfreq = {} #Menciones de cada palabra en cada documento\\nlength = {}  #Número de palabras de cada documento\\ncount = {}  #Número de documentos que contienen cada la palabra\\nlemma = WordNetLemmatizer()\\nfor r in df_review[\"text\"]:\\n    txt = word_tokenize(r.lower())\\n    length[r] = len(txt)\\n    checked = []  #Palabras del documento ya introducidas en count\\n    freqs = {}  #Menciones de cada palabra en el documento\\n    for w in txt:\\n        l = lemma.lemmatize(w)\\n        if not l in count:\\n            count[l] = 0\\n        if not l in checked:\\n            count[l] += 1\\n            checked.append(l)\\n        if not l in bag:\\n            bag[l] = 0\\n        bag[l] += 1\\n        if not l in freqs:\\n            freqs[l] = 0\\n        freqs[l] += 1\\n    freq[r] = freqs\\nprint(sum(length.values())/len(length.values()))\\nprint(len(bag))'"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "bag = {}  #Menciones totales de cada palabra\n",
    "freq = {} #Menciones de cada palabra en cada documento\n",
    "length = {}  #Número de palabras de cada documento\n",
    "count = {}  #Número de documentos que contienen cada la palabra\n",
    "lemma = WordNetLemmatizer()\n",
    "for r in df_review[\"text\"]:\n",
    "    txt = word_tokenize(r.lower())\n",
    "    length[r] = len(txt)\n",
    "    checked = []  #Palabras del documento ya introducidas en count\n",
    "    freqs = {}  #Menciones de cada palabra en el documento\n",
    "    for w in txt:\n",
    "        l = lemma.lemmatize(w)\n",
    "        if not l in count:\n",
    "            count[l] = 0\n",
    "        if not l in checked:\n",
    "            count[l] += 1\n",
    "            checked.append(l)\n",
    "        if not l in bag:\n",
    "            bag[l] = 0\n",
    "        bag[l] += 1\n",
    "        if not l in freqs:\n",
    "            freqs[l] = 0\n",
    "        freqs[l] += 1\n",
    "    freq[r] = freqs\n",
    "print(sum(length.values())/len(length.values()))\n",
    "print(len(bag))'''"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "'import math\\nlist = {}\\nfor i in range(0, len(length)-1):\\n    txt = df_review[\"text\"][i].lower()\\n    for w in bag.keys():\\n        tf = freq[txt][w]/length[txt]\\n        idf = math.log10(len(length)/count[w])\\n        if not w in list:\\n            list[w] = []\\n        list[w].append(tf*idf)\\nfor w in bag.keys():\\n    df_review[w] = list'"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import math\n",
    "list = {}\n",
    "for i in range(0, len(length)-1):\n",
    "    txt = df_review[\"text\"][i].lower()\n",
    "    for w in bag.keys():\n",
    "        tf = freq[txt][w]/length[txt]\n",
    "        idf = math.log10(len(length)/count[w])\n",
    "        if not w in list:\n",
    "            list[w] = []\n",
    "        list[w].append(tf*idf)\n",
    "for w in bag.keys():\n",
    "    df_review[w] = list'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 22904)\t0.1373833046763468\n",
      "  (0, 6009)\t0.0666710987458892\n",
      "  (0, 8801)\t0.06042381731209967\n",
      "  (0, 31673)\t0.22324645070650057\n",
      "  (0, 43453)\t0.19642763884648456\n",
      "  (0, 9526)\t0.16139706624695224\n",
      "  (0, 29714)\t0.11916114244009669\n",
      "  (0, 17875)\t0.1944735485655492\n",
      "  (0, 907)\t0.19862444430735715\n",
      "  (0, 11105)\t0.06147522731027972\n",
      "  (0, 16546)\t0.08008879733556\n",
      "  (0, 62412)\t0.15267243428586294\n",
      "  (0, 18625)\t0.15145333303193056\n",
      "  (0, 66778)\t0.49478172133310416\n",
      "  (0, 3071)\t0.24739086066655208\n",
      "  (0, 18385)\t0.13733438652720664\n",
      "  (0, 88625)\t0.03745306283856678\n",
      "  (0, 60116)\t0.2193646113173283\n",
      "  (0, 87539)\t0.06419157089647097\n",
      "  (0, 40811)\t0.11443690830471603\n",
      "  (0, 4679)\t0.1182682302524849\n",
      "  (0, 23829)\t0.25447431968020345\n",
      "  (0, 87729)\t0.11571470303354955\n",
      "  (0, 3573)\t0.36236867784570664\n",
      "  (0, 63398)\t0.0920406734672639\n",
      "  :\t:\n",
      "  (199999, 82448)\t0.1349482381597636\n",
      "  (199999, 83109)\t0.06407328348191806\n",
      "  (199999, 9908)\t0.05264951177492185\n",
      "  (199999, 4737)\t0.10455735200612472\n",
      "  (199999, 42248)\t0.04938134089706876\n",
      "  (199999, 91340)\t0.10514860877068745\n",
      "  (199999, 15935)\t0.061635151586629154\n",
      "  (199999, 11275)\t0.0627228493119083\n",
      "  (199999, 5278)\t0.1359386472498033\n",
      "  (199999, 46579)\t0.03289712500980404\n",
      "  (199999, 77995)\t0.07520994149019747\n",
      "  (199999, 96235)\t0.054872559764157405\n",
      "  (199999, 55760)\t0.05138803474890788\n",
      "  (199999, 46686)\t0.03212997181830081\n",
      "  (199999, 95511)\t0.06334420256137702\n",
      "  (199999, 62378)\t0.052587762674638\n",
      "  (199999, 94580)\t0.03258274818385594\n",
      "  (199999, 21082)\t0.06352850454313501\n",
      "  (199999, 6850)\t0.14423697148792983\n",
      "  (199999, 61889)\t0.09742045410544942\n",
      "  (199999, 6009)\t0.04972765290176601\n",
      "  (199999, 16546)\t0.059735447444807126\n",
      "  (199999, 87539)\t0.04787825934780521\n",
      "  (199999, 87882)\t0.03629870562600839\n",
      "  (199999, 35778)\t0.03235055413427406\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}