{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "#from sklearn.decomposition import PCA\n",
    "#from sklearn.cluster import KMeans\n",
    "\n",
    "inline_rc = dict(mpl.rcParams)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CARGAMOS LOS DATOS DE yelp_academic_dataset_review.json"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                review_id                 user_id             business_id  \\\n",
      "0  KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA  XQfwVwDr-v0ZS3_CbbE5Xw   \n",
      "1  BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q  7ATYjTIgM3jUlt4UM3IypQ   \n",
      "2  saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A  YjUWPpI6HXG530lwP-fb2A   \n",
      "3  AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA   \n",
      "4  Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ  e4Vwtrqf-wpJfwesgvdgxQ   \n",
      "\n",
      "   stars  useful  funny  cool  \\\n",
      "0    3.0       0      0     0   \n",
      "1    5.0       1      0     1   \n",
      "2    3.0       0      0     0   \n",
      "3    5.0       1      0     1   \n",
      "4    4.0       1      0     1   \n",
      "\n",
      "                                                text                 date  \n",
      "0  If you decide to eat here, just be aware it is...  2018-07-07 22:09:11  \n",
      "1  I've taken a lot of spin classes over the year...  2012-01-03 15:28:18  \n",
      "2  Family diner. Had the buffet. Eclectic assortm...  2014-02-05 20:30:30  \n",
      "3  Wow!  Yummy, different,  delicious.   Our favo...  2015-01-04 00:01:03  \n",
      "4  Cute interior and owner (?) gave us tour of up...  2017-01-14 20:54:15  \n"
     ]
    }
   ],
   "source": [
    "reviews = []\n",
    "with open('data/yelp_academic_dataset_review.json', encoding=\"ansi\") as fl:\n",
    "    i=0\n",
    "    for review in fl:\n",
    "        reviews.append(json.loads(review))\n",
    "        i+=1\n",
    "        if i + 1 > 10000:\n",
    "            break\n",
    "\n",
    "df_review = pd.DataFrame(reviews)\n",
    "print(df_review.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CARGAMOS LOS DATOS DE yelp_academic_dataset_business.json"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              business_id                      name  \\\n",
      "0  Pns2l4eNsfO8kk83dixA6A  Abby Rappoport, LAC, CMQ   \n",
      "1  mpf3x-BjTdTEA3yCZrAYPw             The UPS Store   \n",
      "2  tUFrWirKiKi_TAnsVWINQQ                    Target   \n",
      "3  MTSW4McQd7CbVtyjqoe9mw        St Honore Pastries   \n",
      "4  mWMc6_wTdE0EUBKIGXDVfA  Perkiomen Valley Brewery   \n",
      "\n",
      "                           address           city state postal_code  \\\n",
      "0           1616 Chapala St, Ste 2  Santa Barbara    CA       93101   \n",
      "1  87 Grasso Plaza Shopping Center         Affton    MO       63123   \n",
      "2             5255 E Broadway Blvd         Tucson    AZ       85711   \n",
      "3                      935 Race St   Philadelphia    PA       19107   \n",
      "4                    101 Walnut St     Green Lane    PA       18054   \n",
      "\n",
      "    latitude   longitude  stars  review_count  is_open  \\\n",
      "0  34.426679 -119.711197    5.0             7        0   \n",
      "1  38.551126  -90.335695    3.0            15        1   \n",
      "2  32.223236 -110.880452    3.5            22        0   \n",
      "3  39.955505  -75.155564    4.0            80        1   \n",
      "4  40.338183  -75.471659    4.5            13        1   \n",
      "\n",
      "                                          attributes  \\\n",
      "0                      {'ByAppointmentOnly': 'True'}   \n",
      "1             {'BusinessAcceptsCreditCards': 'True'}   \n",
      "2  {'BikeParking': 'True', 'BusinessAcceptsCredit...   \n",
      "3  {'RestaurantsDelivery': 'False', 'OutdoorSeati...   \n",
      "4  {'BusinessAcceptsCreditCards': 'True', 'Wheelc...   \n",
      "\n",
      "                                          categories  \\\n",
      "0  Doctors, Traditional Chinese Medicine, Naturop...   \n",
      "1  Shipping Centers, Local Services, Notaries, Ma...   \n",
      "2  Department Stores, Shopping, Fashion, Home & G...   \n",
      "3  Restaurants, Food, Bubble Tea, Coffee & Tea, B...   \n",
      "4                          Brewpubs, Breweries, Food   \n",
      "\n",
      "                                               hours  \n",
      "0                                               None  \n",
      "1  {'Monday': '0:0-0:0', 'Tuesday': '8:0-18:30', ...  \n",
      "2  {'Monday': '8:0-22:0', 'Tuesday': '8:0-22:0', ...  \n",
      "3  {'Monday': '7:0-20:0', 'Tuesday': '7:0-20:0', ...  \n",
      "4  {'Wednesday': '14:0-22:0', 'Thursday': '16:0-2...  \n"
     ]
    }
   ],
   "source": [
    "business = []\n",
    "with open('data/yelp_academic_dataset_business.json', encoding=\"ansi\") as fl:\n",
    "    for i, line in enumerate(fl):\n",
    "        business.append(json.loads(line))\n",
    "\n",
    "df_business = pd.DataFrame(business)\n",
    "print(df_business.head())\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ELIMINAMOS LAS COLUMNAS INNECESARIAS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "COMPROBAMOS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "df_review = df_review.drop([\"review_id\", \"user_id\",\"business_id\", \"useful\", \"funny\", \"cool\", \"date\",\"stars\"], axis=1)\n",
    "#df_business = df_business.drop([\"name\", \"address\", \"city\", \"state\", \"postal_code\", \"latitude\", \"longitude\", \"stars\", \"review_count\", \"is_open\", \"attributes\", \"hours\"], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  If you decide to eat here, just be aware it is...\n",
      "1  I've taken a lot of spin classes over the year...\n",
      "2  Family diner. Had the buffet. Eclectic assortm...\n",
      "3  Wow!  Yummy, different,  delicious.   Our favo...\n",
      "4  Cute interior and owner (?) gave us tour of up...\n"
     ]
    }
   ],
   "source": [
    "print(df_review.head())\n",
    "#print(df_business.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 15651)\t0.2104454480765181\n",
      "  (0, 6204)\t0.1085558893661828\n",
      "  (0, 13811)\t0.08076084077667292\n",
      "  (0, 21482)\t0.11805926971455105\n",
      "  (0, 13709)\t0.048597114963181025\n",
      "  (0, 16403)\t0.07746435520037842\n",
      "  (0, 13800)\t0.06881285426357246\n",
      "  (0, 5895)\t0.14191164888090443\n",
      "  (0, 1245)\t0.09584477145921888\n",
      "  (0, 7906)\t0.0397741275911931\n",
      "  (0, 13787)\t0.19787959829199261\n",
      "  (0, 21294)\t0.11570871267253331\n",
      "  (0, 21441)\t0.0869712126229421\n",
      "  (0, 18434)\t0.13848425307602263\n",
      "  (0, 21568)\t0.09891737138495944\n",
      "  (0, 12004)\t0.09493387853267408\n",
      "  (0, 20014)\t0.15421211532850956\n",
      "  (0, 14887)\t0.12846249133663337\n",
      "  (0, 20886)\t0.22486024628977294\n",
      "  (0, 22021)\t0.14533499114322485\n",
      "  (0, 21300)\t0.17080797695753283\n",
      "  (0, 13891)\t0.061558115464438116\n",
      "  (0, 4377)\t0.0801217498073576\n",
      "  (0, 11640)\t0.19381874015665979\n",
      "  (0, 21070)\t0.11652868546031336\n",
      "  :\t:\n",
      "  (9999, 15870)\t0.12767739259830438\n",
      "  (9999, 21669)\t0.061965815037874114\n",
      "  (9999, 1435)\t0.08000467750412456\n",
      "  (9999, 8598)\t0.05835544214372858\n",
      "  (9999, 14824)\t0.046034746199690106\n",
      "  (9999, 22026)\t0.07036017869178075\n",
      "  (9999, 21565)\t0.06174792756476608\n",
      "  (9999, 19765)\t0.04029843941747238\n",
      "  (9999, 3310)\t0.0657885573130496\n",
      "  (9999, 1601)\t0.049879854749031255\n",
      "  (9999, 13939)\t0.07517581822354304\n",
      "  (9999, 13617)\t0.07116143252585332\n",
      "  (9999, 7906)\t0.035957831870763006\n",
      "  (9999, 13891)\t0.055651663536207915\n",
      "  (9999, 3147)\t0.041252208801207495\n",
      "  (9999, 19700)\t0.20547888374500684\n",
      "  (9999, 1184)\t0.05211865613329585\n",
      "  (9999, 21340)\t0.08169424538021255\n",
      "  (9999, 2112)\t0.07011770476482854\n",
      "  (9999, 10404)\t0.1798492017584153\n",
      "  (9999, 10420)\t0.03492451285753466\n",
      "  (9999, 2070)\t0.05133922846429501\n",
      "  (9999, 19941)\t0.12276410360421586\n",
      "  (9999, 22018)\t0.09253969409879335\n",
      "  (9999, 9841)\t0.055485125291819384\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "X = []\n",
    "for r in df_review[\"text\"]: #Por cada review\n",
    "    txt = word_tokenize(r.lower()) #Separa la review en palabras\n",
    "    newtxt = \"\"\n",
    "    for w in txt: #Por cada palabra en txt\n",
    "        l = lemma.lemmatize(w) #se hace lo de quitar mayusculas y quitar raice y eso\n",
    "        newtxt += l+\" \"\n",
    "    X.append(newtxt)\n",
    "\n",
    "X = vectorizer.fit_transform(X)\n",
    "print(X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "X= X.toarray()\n",
    "#X=pd.DataFrame(X)\n",
    "#X=np.array(X, dtype=object)\n",
    "print(X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim originally:  (10000, 22189)\n",
      "Dim after PCA:  (10000, 2)\n",
      "[[ 0.11003364  0.11166316]\n",
      " [-0.05171281  0.12324963]\n",
      " [-0.10889372 -0.06413692]\n",
      " ...\n",
      " [-0.02617894  0.06520898]\n",
      " [ 0.0820722  -0.08249784]\n",
      " [-0.00582602  0.15804222]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Antes de aplicar el método de clasificación utilizamos PCA para reducir el número de atributos\n",
    "print('Dim originally: ',X.shape)\n",
    "#Como vamos a representar gráficamente los clusters, nos quedaremos con los 2 atributos más imporantes\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "# Cambio de base a dos dimensiones PCA\n",
    "X = pca.transform(X)\n",
    "print('Dim after PCA: ',X.shape)\n",
    "print(X)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "#X = pd.DataFrame(X)\n",
    "#print(X[0][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "class Distance_computation_grid(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def compute_distance(self,samples):\n",
    "\n",
    "        Distance_mat = np.zeros((len(samples),len(samples))) #Hace una matriz de 10000*10000\n",
    "        for i in range(Distance_mat.shape[0]):\n",
    "            for j in range(Distance_mat.shape[0]):\n",
    "                if i!=j:#Solo calcula distancias en una mitad de la matriz\n",
    "                    Distance_mat[i,j] = float(self.distance_calculate(samples[i],samples[j])) #Para cada pos en la matriz calcula las distancias\n",
    "\n",
    "                else:\n",
    "                    Distance_mat[i,j] = 10**4 #Pone un valor muy grande\n",
    "        return Distance_mat\n",
    "\n",
    "\n",
    "    def distance_calculate(self,sample1,sample2):\n",
    "\n",
    "        dist = []\n",
    "        for i in range(len(sample1)):\n",
    "            for j in range(len(sample2)):\n",
    "                    dist.append(abs(sample1[i][0]-sample2[j][0])+abs(sample1[i][1]-sample2[j][1]))\n",
    "        return np.min(dist)\n",
    "\n",
    "\n",
    "    def intersampledist(self,s1,s2):\n",
    "        if str(type(s2[0]))!='<class \\'list\\'>':\n",
    "            s2=[s2]\n",
    "        if str(type(s1[0]))!='<class \\'list\\'>':\n",
    "            s1=[s1]\n",
    "        m = len(s1)\n",
    "        n = len(s2)\n",
    "        dist = []\n",
    "        if n>=m:\n",
    "            for i in range(n):\n",
    "                for j in range(m):\n",
    "                    if (len(s2[i])>=len(s1[j])) and str(type(s2[i][0])!='<class \\'list\\'>'):\n",
    "                        dist.append(self.interclusterdist(s2[i],s1[j]))\n",
    "                    else:\n",
    "                        dist.append(abs(s1[i][0]-s2[j][0])+abs(s1[i][1]-s2[j][1]))\n",
    "        else:\n",
    "            for i in range(m):\n",
    "                for j in range(n):\n",
    "                    if (len(s1[i])>=len(s2[j])) and str(type(s1[i][0])!='<class \\'list\\'>'):\n",
    "                        dist.append(self.interclusterdist(s1[i],s2[j]))\n",
    "                    else:\n",
    "                        dist.append(abs(s1[i][0]-s2[j][0])+abs(s1[i][1]-s2[j][1]))\n",
    "        return min(dist)\n",
    "\n",
    "    def centroide(self,sample):\n",
    "        if(len(sample) == 1):\n",
    "            return np.mean()\n",
    "        else:\n",
    "            sample1=sample[0]\n",
    "            i=1\n",
    "            while i < len(sample):\n",
    "                dist.append(abs(s1[i][0]-s2[j][0])+abs(s1[i][1]-s2[j][1]))\n",
    "\n",
    "        return np.mean(dist)\n",
    "\n",
    "    def interclusterdist(self,cl,sample):\n",
    "        if sample[0]!='<class \\'list\\'>':\n",
    "            sample = [sample]\n",
    "        dist   = []\n",
    "        for i in range(len(cl)):\n",
    "            for j in range(len(sample)):\n",
    "                dist.append(np.linalg.norm(np.array(cl[i])-np.array(sample[j])))\n",
    "        return min(dist)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11003364 0.11166316]\n",
      "[-0.05171281  0.12324963]\n",
      "[ 0.16174644 -0.01158647]\n",
      "[0.17333291304338522]\n"
     ]
    }
   ],
   "source": [
    "dist = []\n",
    "print(X[0])\n",
    "print(X[1])\n",
    "print((np.array(X[0])-np.array(X[1])))\n",
    "dist.append(abs(X[0][0]-X[1][0])+abs(X[0][1]-X[1][1]))\n",
    "print(dist)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size before clustering    :-  10000\n",
      "[[1.00000000e+04 1.73332913e-01 3.94727431e-01 ... 1.82666752e-01\n",
      "  2.22122434e-01 1.62238721e-01]\n",
      " [1.73332913e-01 1.00000000e+04 2.44567455e-01 ... 8.35745127e-02\n",
      "  3.39532470e-01 8.06793877e-02]\n",
      " [3.94727431e-01 2.44567455e-01 1.00000000e+04 ... 2.12060679e-01\n",
      "  2.09326830e-01 3.25246843e-01]\n",
      " ...\n",
      " [1.82666752e-01 8.35745127e-02 2.12060679e-01 ... 1.00000000e+04\n",
      "  2.55957957e-01 1.13186164e-01]\n",
      " [2.22122434e-01 3.39532470e-01 2.09326830e-01 ... 2.55957957e-01\n",
      "  1.00000000e+04 3.28438278e-01]\n",
      " [1.62238721e-01 8.06793877e-02 3.25246843e-01 ... 1.13186164e-01\n",
      "  3.28438278e-01 1.00000000e+04]]\n"
     ]
    }
   ],
   "source": [
    "progression = [[i] for i in range(X.shape[0])]\n",
    "samples     = [[list(X[i])] for i in range(X.shape[0])]\n",
    "m = len(samples)\n",
    "distcal  = Distance_computation_grid()\n",
    "\n",
    "print('Sample size before clustering    :- ',m)\n",
    "Distance_mat      = distcal.compute_distance(samples)\n",
    "print(Distance_mat)\n",
    "dist_mat = pd.DataFrame(Distance_mat)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8917517534883244e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def buscarMin(matrizDistancias):\n",
    "    columnas=0\n",
    "    filas=0\n",
    "    x= 0\n",
    "    y=0\n",
    "    min =1000000000000000000000000000000000000000000\n",
    "    columnas=len(matrizDistancias)\n",
    "    filas=len(matrizDistancias)\n",
    "\n",
    "    for i in range(0,len(matrizDistancias)-1):\n",
    "        for j in range(0,len(matrizDistancias)-1):\n",
    "            if matrizDistancias[i][j] < min:\n",
    "                x=i\n",
    "                y=j\n",
    "                min = matrizDistancias[i][j]\n",
    "\n",
    "    return x,y, min\n",
    "\n",
    "print(dist_mat.min().min().())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('se han calculado las distancias')\n",
    "sample_ind_needed = np.where(Distance_mat==Distance_mat.min())[0]\n",
    "value_to_add      = samples.pop(sample_ind_needed[1])\n",
    "samples[sample_ind_needed[0]].append(value_to_add)\n",
    "\n",
    "print('Cluster Node 1                   :-',progression[sample_ind_needed[0]])\n",
    "print('Cluster Node 2                   :-',progression[sample_ind_needed[1]])\n",
    "\n",
    "progression[sample_ind_needed[0]].append(progression[sample_ind_needed[1]])\n",
    "progression[sample_ind_needed[0]] = [progression[sample_ind_needed[0]]]\n",
    "v = progression.pop(sample_ind_needed[1])\n",
    "m = len(samples)\n",
    "\n",
    "print('Progression(Current Sample)      :-',progression)\n",
    "print('Cluster attained                 :-',progression[sample_ind_needed[0]])\n",
    "print('Sample size after clustering     :-',m)\n",
    "print('\\n')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "def hierarchical_clustering(data,linkage,no_of_clusters):\n",
    "    #first step is to calculate the initial distance matrix\n",
    "    #it consists distances from all the point to all the point\n",
    "    color = ['r','g','b','y','c','m','k','w']\n",
    "    initial_distances = pairwise_distances(data,metric='euclidean')\n",
    "    #making all the diagonal elements infinity\n",
    "    np.fill_diagonal(initial_distances,sys.maxsize)\n",
    "    clusters = find_clusters(initial_distances,linkage)\n",
    "\n",
    "    #plotting the clusters\n",
    "    iteration_number = initial_distances.shape[0] - no_of_clusters\n",
    "    clusters_to_plot = clusters[iteration_number]\n",
    "    arr = np.unique(clusters_to_plot)\n",
    "\n",
    "    indices_to_plot = []\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Scatter Plot for clusters')\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    for x in np.nditer(arr):\n",
    "        indices_to_plot.append(np.where(clusters_to_plot==x))\n",
    "    p=0\n",
    "\n",
    "    print(clusters_to_plot)\n",
    "    for i in range(0,len(indices_to_plot)):\n",
    "        for j in np.nditer(indices_to_plot[i]):\n",
    "               ax.scatter(data[j,0],data[j,1], c= color[p])\n",
    "        p = p + 1\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def find_clusters(input,linkage):\n",
    "    clusters = {}\n",
    "    row_index = -1\n",
    "    col_index = -1\n",
    "    array = []\n",
    "\n",
    "\n",
    "    for n in range(input.shape[0]):\n",
    "        array.append(n)\n",
    "\n",
    "    clusters[0] = array.copy()\n",
    "\n",
    "    #finding minimum value from the distance matrix\n",
    "    #note that this loop will always return minimum value from bottom triangle of matrix\n",
    "    for k in range(1, input.shape[0]):\n",
    "        min_val = sys.maxsize\n",
    "\n",
    "        for i in range(0, input.shape[0]):\n",
    "            for j in range(0, input.shape[1]):\n",
    "                if(input[i][j]<=min_val):\n",
    "                    min_val = input[i][j]\n",
    "                    row_index = i\n",
    "                    col_index = j\n",
    "\n",
    "        #once we find the minimum value, we need to update the distance matrix\n",
    "        #updating the matrix by calculating the new distances from the cluster to all points\n",
    "\n",
    "        #for Single Linkage\n",
    "        if(linkage == \"single\" or linkage ==\"Single\"):\n",
    "            for i in range(0,input.shape[0]):\n",
    "                if(i != col_index):\n",
    "                    #we calculate the distance of every data point from newly formed cluster and update the matrix.\n",
    "                    temp = min(input[col_index][i],input[row_index][i])\n",
    "                    #we update the matrix symmetrically as our distance matrix should always be symmetric\n",
    "                    input[col_index][i] = temp\n",
    "                    input[i][col_index] = temp\n",
    "        #for Complete Linkage\n",
    "        elif(linkage==\"Complete\" or linkage == \"complete\"):\n",
    "             for i in range(0,input.shape[0]):\n",
    "                if(i != col_index and i!=row_index):\n",
    "                    temp = min(input[col_index][i],input[row_index][i])\n",
    "                    input[col_index][i] = temp\n",
    "                    input[i][col_index] = temp\n",
    "        #for Average Linkage\n",
    "        elif(linkage==\"Average\" or linkage == \"average\"):\n",
    "             for i in range(0,input.shape[0]):\n",
    "                if(i != col_index and i!=row_index):\n",
    "                    temp = (input[col_index][i]+input[row_index][i])/2\n",
    "                    input[col_index][i] = temp\n",
    "                    input[i][col_index] = temp\n",
    "\n",
    "        elif(linkage==\"Centroid\" or linkage ==\"centroid\"):\n",
    "            for i in range(0,input.shape[0]):\n",
    "                if(i!=col_index and i!=row_index):\n",
    "                    dist_centroid = cal_dist_from_centroid(i,row_index,col_index)\n",
    "                    input[col_index][i] = dist_centroid\n",
    "                    input[i][col_index] = dist_centroid\n",
    "\n",
    "        #set the rows and columns for the cluster with higher index i.e. the row index to infinity\n",
    "        #Set input[row_index][for_all_i] = infinity\n",
    "        #set input[for_all_i][row_index] = infinity\n",
    "        for i in range (0,input.shape[0]):\n",
    "            input[row_index][i] = sys.maxsize\n",
    "            input[i][row_index] = sys.maxsize\n",
    "\n",
    "        #Manipulating the dictionary to keep track of cluster formation in each step\n",
    "        #if k=0,then all datapoints are clusters\n",
    "\n",
    "        minimum = min(row_index,col_index)\n",
    "        maximum = max(row_index,col_index)\n",
    "        for n in range(len(array)):\n",
    "            if(array[n]==maximum):\n",
    "                array[n] = minimum\n",
    "        clusters[k] = array.copy()\n",
    "\n",
    "    return clusters\n",
    "clusters = hierarchical_clustering(X,1,2)\n",
    "print(clusters)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import the f\n",
    "print(clusters)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cluster and linkage functions\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "\n",
    "# Use the linkage() function\n",
    "distance_matrix = linkage(X, method = 'ward', metric = 'euclidean')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import the dendrogram function\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "# Create a dendrogram\n",
    "dn = dendrogram(distance_matrix)\n",
    "\n",
    "# Display the dendogram\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
