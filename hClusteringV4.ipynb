{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "#from sklearn.decomposition import PCA\n",
    "#from sklearn.cluster import KMeans\n",
    "\n",
    "inline_rc = dict(mpl.rcParams)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CARGAMOS LOS DATOS DE yelp_academic_dataset_review.json"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/datosExperimento.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m reviews \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdata/datosExperimento.json\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mansi\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m fl:\n\u001B[1;32m      3\u001B[0m     i\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m review \u001B[38;5;129;01min\u001B[39;00m fl:\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data/datosExperimento.json'"
     ]
    }
   ],
   "source": [
    "reviews = []\n",
    "with open('data/datosExperimento.json', encoding=\"ansi\") as fl:\n",
    "    i=0\n",
    "    for review in fl:\n",
    "        reviews.append(json.loads(review))\n",
    "        i+=1\n",
    "        if i + 1 > 5:\n",
    "            break\n",
    "\n",
    "df_review = pd.DataFrame(reviews)\n",
    "print(df_review.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CARGAMOS LOS DATOS DE yelp_academic_dataset_business.json"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "business = []\n",
    "with open('data/yelp_academic_dataset_business.json', encoding=\"ansi\") as fl:\n",
    "    for i, line in enumerate(fl):\n",
    "        business.append(json.loads(line))\n",
    "\n",
    "df_business = pd.DataFrame(business)\n",
    "print(df_business.head())\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ELIMINAMOS LAS COLUMNAS INNECESARIAS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "COMPROBAMOS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_review = df_review.drop([\"review_id\", \"user_id\",\"business_id\", \"useful\", \"funny\", \"cool\", \"date\",\"stars\"], axis=1)\n",
    "#df_business = df_business.drop([\"name\", \"address\", \"city\", \"state\", \"postal_code\", \"latitude\", \"longitude\", \"stars\", \"review_count\", \"is_open\", \"attributes\", \"hours\"], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(df_review.head())\n",
    "#print(df_business.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "X = []\n",
    "for r in df_review[\"text\"]: #Por cada review\n",
    "    txt = word_tokenize(r.lower()) #Separa la review en palabras\n",
    "    newtxt = \"\"\n",
    "    for w in txt: #Por cada palabra en txt\n",
    "        l = lemma.lemmatize(w) #se hace lo de quitar mayusculas y quitar raice y eso\n",
    "        newtxt += l+\" \"\n",
    "    X.append(newtxt)\n",
    "\n",
    "X = vectorizer.fit_transform(X)\n",
    "print(X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X= X.toarray()\n",
    "X=pd.DataFrame(X)\n",
    "#X=np.array(X, dtype=object)\n",
    "print(X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Antes de aplicar el método de clasificación utilizamos PCA para reducir el número de atributos\n",
    "print('Dim originally: ',X.shape)\n",
    "#Como vamos a representar gráficamente los clusters, nos quedaremos con los 2 atributos más imporantes\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "# Cambio de base a dos dimensiones PCA\n",
    "X = pca.transform(X)\n",
    "print('Dim after PCA: ',X.shape)\n",
    "print(X)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#X = pd.DataFrame(X)\n",
    "#print(X[0][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Distance_computation_grid(object):\n",
    "\n",
    "#De aquí por ahora solo utilizamos compute distant y distance calculate para solamente crear la matriz de distancias\n",
    "#Para simplificar un poco lo de juntar instancias hago la matriz entera llena\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def compute_distance(self,samples):\n",
    "\n",
    "        Distance_mat = np.zeros((len(samples),len(samples))) #Hace una matriz de 10000*10000\n",
    "        for i in range(Distance_mat.shape[0]):\n",
    "            for j in range(Distance_mat.shape[0]):\n",
    "                if i!=j and i < j:#Solo calcula distancias en una mitad de la matriz\n",
    "                    distancia = float(self.distance_calculate(samples[i],samples[j])) #Distancia entrela muestra i e j\n",
    "                    Distance_mat[i,j] = distancia #Meto en la matriz la distancia do vece\n",
    "                    Distance_mat[j,i] = distancia\n",
    "                elif i ==j:\n",
    "                    Distance_mat[i,j] = 10**4 #Pone un valor muy grande para que nunca salga como la distancia más pequeña entrela misma instancia\n",
    "        return Distance_mat\n",
    "\n",
    "\n",
    "    def distance_calculate(self,sample1,sample2):\n",
    "\n",
    "        dist = []\n",
    "        for i in range(len(sample1)):\n",
    "            for j in range(len(sample2)):\n",
    "                    dist.append(abs(sample1[i][0]-sample2[j][0])+abs(sample1[i][1]-sample2[j][1]))\n",
    "        return np.min(dist)\n",
    "\n",
    "\n",
    "#Esto no se usa para nada lo he dejado más que nada por si había algo que nos interesaba de aquí.\n",
    "\n",
    "    def intersampledist(self,s1,s2):\n",
    "        if str(type(s2[0]))!='<class \\'list\\'>':\n",
    "            s2=[s2]\n",
    "        if str(type(s1[0]))!='<class \\'list\\'>':\n",
    "            s1=[s1]\n",
    "        m = len(s1)\n",
    "        n = len(s2)\n",
    "        dist = []\n",
    "        if n>=m:\n",
    "            for i in range(n):\n",
    "                for j in range(m):\n",
    "                    if (len(s2[i])>=len(s1[j])) and str(type(s2[i][0])!='<class \\'list\\'>'):\n",
    "                        dist.append(self.interclusterdist(s2[i],s1[j]))\n",
    "                    else:\n",
    "                        dist.append(abs(s1[i][0]-s2[j][0])+abs(s1[i][1]-s2[j][1]))\n",
    "        else:\n",
    "            for i in range(m):\n",
    "                for j in range(n):\n",
    "                    if (len(s1[i])>=len(s2[j])) and str(type(s1[i][0])!='<class \\'list\\'>'):\n",
    "                        dist.append(self.interclusterdist(s1[i],s2[j]))\n",
    "                    else:\n",
    "                        dist.append(abs(s1[i][0]-s2[j][0])+abs(s1[i][1]-s2[j][1]))\n",
    "        return min(dist)\n",
    "\n",
    "    def centroide(self,sample):\n",
    "        if(len(sample) == 1):\n",
    "            return np.mean()\n",
    "        else:\n",
    "            sample1=sample[0]\n",
    "            i=1\n",
    "            while i < len(sample):\n",
    "                dist.append(abs(s1[i][0]-s2[j][0])+abs(s1[i][1]-s2[j][1]))\n",
    "\n",
    "        return np.mean(dist)\n",
    "\n",
    "    def interclusterdist(self,cl,sample):\n",
    "        if sample[0]!='<class \\'list\\'>':\n",
    "            sample = [sample]\n",
    "        dist   = []\n",
    "        for i in range(len(cl)):\n",
    "            for j in range(len(sample)):\n",
    "                dist.append(np.linalg.norm(np.array(cl[i])-np.array(sample[j])))\n",
    "        return min(dist)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Aquí básicamente se hace la matriz de distancias osea aquí no creo que e cambie nada\n",
    "progression = [[i] for i in range(X.shape[0])]\n",
    "samples     = [[list(X[i])] for i in range(X.shape[0])]\n",
    "m = len(samples)\n",
    "distcal  = Distance_computation_grid()\n",
    "\n",
    "print('Sample size before clustering    :- ',m)\n",
    "Distance_mat      = distcal.compute_distance(samples)\n",
    "print(Distance_mat)\n",
    "dist_mat = pd.DataFrame(Distance_mat)\n",
    "print(dist_mat)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Aquí se busca la distancia más pequeña y se devuelven los índices\n",
    "minimos=np.matrix(dist_mat).argmin(axis = 1).tolist() #coordenadas x del valor minimo de cada fila. En dataframe es x,y, es decir en elprimer [] van las filas y en el segun do [] van las columnas\n",
    "print(minimos)#Aquí te imprime las coordenadas de cada fila para que veas que formato tiene\n",
    "minn=dist_mat[minimos[0][0]][0] #Aquí simplemente guardo el minimo de la primera fila, los [][] en minimos es porque argmin devuelve un array de arrays\n",
    "print(minn)\n",
    "y=0 #la coordenada y del elemento más pequeño de la primera fila\n",
    "x=minimos[0][0] #la coordenada x del elemento más pequeño de la primera fila\n",
    "for i in range(1,len(minimos)-1): #Para cada minimo en la lista de minimos\n",
    "    if dist_mat[minimos[i][0]][i] < minn: #Si el mínimo de esa fila es más pequeño que el minimo actual\n",
    "        minn=dist_mat[minimos[i][0]][i] #Se guarda el nuevo minimo junto a sus coordenadas.\n",
    "        x=minimos[i][0]\n",
    "        y=i\n",
    "print('---------------------------------')\n",
    "print(x)\n",
    "print(y)\n",
    "print(minn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Aquí lo que estuve haciendo es intentar hacer la primera iteración de la matriz que genere con los primerass 5 instancias de datosExperimentos\n",
    "#Recomiendo ver https://youtu.be/YH0r47m0kFM\n",
    "nodo1 = dist_mat[0] #En el nodo1 guardo la columna 0\n",
    "nodo2 = dist_mat[4] #En el nodo2 guardo la columna 4 (son las coordenadas del mininmo mas que na)\n",
    "print(nodo1)\n",
    "print(nodo2)\n",
    "print(len(dist_mat))\n",
    "eliminado = False #Un booleano para borrar solo una vez el minimo ya que sale do veces la pareja de 10000 y el minimo\n",
    "nuevaFila = [] #Aqui voy a guardar las distancias nuevas obtenidas por complete link, es decir, de cada pareja de distancias te quedas con la más grande\n",
    "for i in range(0,len(dist_mat)): #Para cada pareja de distancias\n",
    "    if((nodo1[i]==minn or nodo2[i]==minn) and not eliminado): #Aquí me ocupo de la pareja de 10000 y el minimo que sale como primera\n",
    "        nuevaFila.append(float(10**4))\n",
    "        eliminado = True\n",
    "    elif((nodo1[i]==minn or nodo2[i]==minn) and eliminado): #Aqui como sale otra vez la pareja 10000 y el minimo no hago nada con ella igual se puede poner los if de otra manera no se me ocurria de ptra manera ayer la verdad\n",
    "        print()\n",
    "    elif(nodo1[i]>=nodo2[i]): #Aqui en los dos siguiientes ifs nos quedamos con las distancias mas grandes de las parejas de distancias\n",
    "            nuevaFila.append(float(nodo1[i]))\n",
    "    else:\n",
    "            nuevaFila.append(float(nodo2[i]))\n",
    "print(pd.DataFrame(nuevaFila)) #Convierto la lista de distancias en un dataframe para poner meterlo a la matriz de ditancias\n",
    "dist_mat = dist_mat.drop([0,4] ,axis=1) #Quito las filas y columnas de las instancias que se han mergeado\n",
    "dist_mat = dist_mat.drop([0,4] ,axis=0)\n",
    "\n",
    "#dist_mat.insert(0,'(1,4)',nuevaFila[1:]) Aquí estan mis intento de meter la nueva fila y columna\n",
    "#dist_mat.loc[len(dist_mat)] = nuevaFila\n",
    "print(dist_mat)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "sample_ind_needed = np.where(Distance_mat==Distance_mat.min())[0]\n",
    "value_to_add      = samples.pop(sample_ind_needed[1])\n",
    "samples[sample_ind_needed[0]].append(value_to_add)\n",
    "\n",
    "print('Cluster Node 1                   :-',progression[sample_ind_needed[0]])\n",
    "print('Cluster Node 2                   :-',progression[sample_ind_needed[1]])\n",
    "\n",
    "progression[sample_ind_needed[0]].append(progression[sample_ind_needed[1]])\n",
    "progression[sample_ind_needed[0]] = [progression[sample_ind_needed[0]]]\n",
    "v = progression.pop(sample_ind_needed[1])\n",
    "m = len(samples)\n",
    "\n",
    "print('Progression(Current Sample)      :-',progression)\n",
    "print('Cluster attained                 :-',progression[sample_ind_needed[0]])\n",
    "print('Sample size after clustering     :-',m)\n",
    "print('\\n')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "def hierarchical_clustering(data,linkage,no_of_clusters):\n",
    "    #first step is to calculate the initial distance matrix\n",
    "    #it consists distances from all the point to all the point\n",
    "    color = ['r','g','b','y','c','m','k','w']\n",
    "    initial_distances = pairwise_distances(data,metric='euclidean')\n",
    "    #making all the diagonal elements infinity\n",
    "    np.fill_diagonal(initial_distances,sys.maxsize)\n",
    "    clusters = find_clusters(initial_distances,linkage)\n",
    "\n",
    "    #plotting the clusters\n",
    "    iteration_number = initial_distances.shape[0] - no_of_clusters\n",
    "    clusters_to_plot = clusters[iteration_number]\n",
    "    arr = np.unique(clusters_to_plot)\n",
    "\n",
    "    indices_to_plot = []\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Scatter Plot for clusters')\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    for x in np.nditer(arr):\n",
    "        indices_to_plot.append(np.where(clusters_to_plot==x))\n",
    "    p=0\n",
    "\n",
    "    print(clusters_to_plot)\n",
    "    for i in range(0,len(indices_to_plot)):\n",
    "        for j in np.nditer(indices_to_plot[i]):\n",
    "               ax.scatter(data[j,0],data[j,1], c= color[p])\n",
    "        p = p + 1\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def find_clusters(input,linkage):\n",
    "    clusters = {}\n",
    "    row_index = -1\n",
    "    col_index = -1\n",
    "    array = []\n",
    "\n",
    "\n",
    "    for n in range(input.shape[0]):\n",
    "        array.append(n)\n",
    "\n",
    "    clusters[0] = array.copy()\n",
    "\n",
    "    #finding minimum value from the distance matrix\n",
    "    #note that this loop will always return minimum value from bottom triangle of matrix\n",
    "    for k in range(1, input.shape[0]):\n",
    "        min_val = sys.maxsize\n",
    "\n",
    "        for i in range(0, input.shape[0]):\n",
    "            for j in range(0, input.shape[1]):\n",
    "                if(input[i][j]<=min_val):\n",
    "                    min_val = input[i][j]\n",
    "                    row_index = i\n",
    "                    col_index = j\n",
    "\n",
    "        #once we find the minimum value, we need to update the distance matrix\n",
    "        #updating the matrix by calculating the new distances from the cluster to all points\n",
    "\n",
    "        #for Single Linkage\n",
    "        if(linkage == \"single\" or linkage ==\"Single\"):\n",
    "            for i in range(0,input.shape[0]):\n",
    "                if(i != col_index):\n",
    "                    #we calculate the distance of every data point from newly formed cluster and update the matrix.\n",
    "                    temp = min(input[col_index][i],input[row_index][i])\n",
    "                    #we update the matrix symmetrically as our distance matrix should always be symmetric\n",
    "                    input[col_index][i] = temp\n",
    "                    input[i][col_index] = temp\n",
    "        #for Complete Linkage\n",
    "        elif(linkage==\"Complete\" or linkage == \"complete\"):\n",
    "             for i in range(0,input.shape[0]):\n",
    "                if(i != col_index and i!=row_index):\n",
    "                    temp = min(input[col_index][i],input[row_index][i])\n",
    "                    input[col_index][i] = temp\n",
    "                    input[i][col_index] = temp\n",
    "        #for Average Linkage\n",
    "        elif(linkage==\"Average\" or linkage == \"average\"):\n",
    "             for i in range(0,input.shape[0]):\n",
    "                if(i != col_index and i!=row_index):\n",
    "                    temp = (input[col_index][i]+input[row_index][i])/2\n",
    "                    input[col_index][i] = temp\n",
    "                    input[i][col_index] = temp\n",
    "\n",
    "        elif(linkage==\"Centroid\" or linkage ==\"centroid\"):\n",
    "            for i in range(0,input.shape[0]):\n",
    "                if(i!=col_index and i!=row_index):\n",
    "                    dist_centroid = cal_dist_from_centroid(i,row_index,col_index)\n",
    "                    input[col_index][i] = dist_centroid\n",
    "                    input[i][col_index] = dist_centroid\n",
    "\n",
    "        #set the rows and columns for the cluster with higher index i.e. the row index to infinity\n",
    "        #Set input[row_index][for_all_i] = infinity\n",
    "        #set input[for_all_i][row_index] = infinity\n",
    "        for i in range (0,input.shape[0]):\n",
    "            input[row_index][i] = sys.maxsize\n",
    "            input[i][row_index] = sys.maxsize\n",
    "\n",
    "        #Manipulating the dictionary to keep track of cluster formation in each step\n",
    "        #if k=0,then all datapoints are clusters\n",
    "\n",
    "        minimum = min(row_index,col_index)\n",
    "        maximum = max(row_index,col_index)\n",
    "        for n in range(len(array)):\n",
    "            if(array[n]==maximum):\n",
    "                array[n] = minimum\n",
    "        clusters[k] = array.copy()\n",
    "\n",
    "    return clusters\n",
    "clusters = hierarchical_clustering(X,1,2)\n",
    "print(clusters)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import the f\n",
    "print(clusters)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cluster and linkage functions\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "\n",
    "# Use the linkage() function\n",
    "distance_matrix = linkage(X, method = 'ward', metric = 'euclidean')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import the dendrogram function\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "# Create a dendrogram\n",
    "dn = dendrogram(distance_matrix)\n",
    "\n",
    "# Display the dendogram\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
